use std::collections::HashMap;
use std::num::NonZeroU64;
use std::panic::{RefUnwindSafe, UnwindSafe};
use std::path::PathBuf;
use std::sync::Arc;
use tokio::sync::Mutex;
use serde_json::{Value, Number, Map};
use zarrs::array::{Array, ArrayBuilder, DataType, FillValue};
use zarrs::array::codec::{GzipCodec, ZstdCodec};
use zarrs::array_subset::ArraySubset;
use zarrs_filesystem::FilesystemStore;

use pumpkin_world::generation::chunk_noise::CHUNK_DIM;

// Safe coordinate offset to handle negative MC coordinates
// Supports radius up to 50,000,000 (50 million chunks)
const SAFE_COORDINATE_OFFSET: i64 = 100_000_000;

type ZarrError = Box<dyn std::error::Error + Send + Sync>;
type ZarrResult<T> = Result<T, ZarrError>;

/// Configuration for Zarr compression
#[derive(Clone, Debug)]
pub enum CompressionType {
    None,
    Gzip { level: u32 },
    Zstd { level: i32 },
}

/// Generation metadata to store with zarr array
#[derive(Clone, Debug)]
pub struct GenerationMetadata {
    pub seed: u64,
    pub center_x: i32,
    pub center_z: i32,
    pub radius_start: i32,
    pub radius_end: i32,
    pub circular: bool,
}

impl Default for CompressionType {
    fn default() -> Self {
        CompressionType::Gzip { level: 6 }
    }
}

/// Simple zarr storage for high-throughput chunk processing
pub struct ZarrBlockStorage {
    array: Arc<Array<FilesystemStore>>,
    zarr_region_size: usize,
    // Coordinate offset to handle negative MC coordinates
    coordinate_offset: i64,
    // Buffer for collecting chunks to write together
    pending_chunks: Arc<Mutex<HashMap<(i32, i32), Vec<i64>>>>,
}

impl UnwindSafe for ZarrBlockStorage {}
impl RefUnwindSafe for ZarrBlockStorage {}

impl ZarrBlockStorage {
    /// Create new storage
    pub async fn new(
        storage_location: impl Into<PathBuf>,
        zarr_region_size: usize,
        compression: Option<CompressionType>,
        generation_metadata: Option<GenerationMetadata>,
    ) -> ZarrResult<Self> {
        let storage_location = storage_location.into();
        let compression = compression.unwrap_or_default();
        
        let store = FilesystemStore::new(&storage_location)?;
        let fill_value = FillValue::new(0i64.to_le_bytes().to_vec());
        let zarr_region_blocks = zarr_region_size * CHUNK_DIM as usize;
        
        let mut builder = ArrayBuilder::new(
            vec![u64::MAX, u64::MAX],
            DataType::Int64,
            vec![
                NonZeroU64::new(zarr_region_blocks as u64).unwrap(),
                NonZeroU64::new(zarr_region_blocks as u64).unwrap()
            ].into(),
            fill_value,
        );
        
        match compression {
            CompressionType::None => {}
            CompressionType::Gzip { level } => {
                let codec = GzipCodec::new(level)?;
                builder.bytes_to_bytes_codecs(vec![Arc::new(codec)]);
            }
            CompressionType::Zstd { level } => {
                let codec = ZstdCodec::new(level, false);
                builder.bytes_to_bytes_codecs(vec![Arc::new(codec)]);
            }
        }
        
        // Add comprehensive metadata
        let mut metadata = std::collections::HashMap::new();
        metadata.insert("description".to_string(), Value::String("Minecraft chunk height maps generated by Ekko Generator".to_string()));
        metadata.insert("data_type".to_string(), Value::String("height_map".to_string()));
        metadata.insert("units".to_string(), Value::String("blocks".to_string()));
        metadata.insert("coordinate_system".to_string(), Value::String("minecraft_chunk_coordinates".to_string()));
        metadata.insert("coordinate_offset".to_string(), Value::Number(Number::from(SAFE_COORDINATE_OFFSET)));
        metadata.insert("zarr_region_size".to_string(), Value::Number(Number::from(zarr_region_size)));
        metadata.insert("chunk_dimension".to_string(), Value::Number(Number::from(CHUNK_DIM)));
        metadata.insert("generation_timestamp".to_string(), Value::String(chrono::Utc::now().to_rfc3339()));
        metadata.insert("generator_version".to_string(), Value::String("ekko-generator-0.1.0".to_string()));
        
        // Add generation parameters if provided
        if let Some(gen_meta) = generation_metadata {
            metadata.insert("seed".to_string(), Value::Number(Number::from(gen_meta.seed)));
            metadata.insert("center_x".to_string(), Value::Number(Number::from(gen_meta.center_x)));
            metadata.insert("center_z".to_string(), Value::Number(Number::from(gen_meta.center_z)));
            metadata.insert("radius_start".to_string(), Value::Number(Number::from(gen_meta.radius_start)));
            metadata.insert("radius_end".to_string(), Value::Number(Number::from(gen_meta.radius_end)));
            metadata.insert("radius_circular".to_string(), Value::Bool(gen_meta.circular));
        }
        
        // Convert HashMap to Map
        let metadata_map: Map<String, Value> = metadata.into_iter().collect();
        println!("   â€¢ Metadata keys: {:?}", metadata_map.keys().collect::<Vec<_>>());
        builder.attributes(metadata_map);
        
        let store_arc = Arc::new(store);
        let array = builder.build(Arc::clone(&store_arc), "/")?;
        
        // Explicitly write array metadata to ensure .zarray file is created
        if let Err(e) = array.store_metadata() {
            eprintln!("Warning: Failed to store array metadata: {}", e);
        }
        
        Ok(Self {
            array: Arc::new(array),
            zarr_region_size,
            coordinate_offset: SAFE_COORDINATE_OFFSET,
            pending_chunks: Arc::new(Mutex::new(HashMap::new())),
        })
    }

    /// Store chunk (no auto-flush - use flush_chunks manually)
    pub async fn store_chunk(
        &self,
        chunk_x: i32,
        chunk_z: i32,
        height_map: Vec<i64>,
    ) -> ZarrResult<()> {
        if height_map.len() != (CHUNK_DIM as usize*CHUNK_DIM as usize) {
            return Err(format!(
                "Height map size mismatch: got {}, expected {} (CHUNK_DIM={}, CHUNK_DIM*CHUNK_DIM={})",
                height_map.len(),
                (CHUNK_DIM as usize * CHUNK_DIM as usize),
                CHUNK_DIM,
                (CHUNK_DIM as usize * CHUNK_DIM as usize)
            ).into());
        }

        let mut pending = self.pending_chunks.lock().await;
        pending.insert((chunk_x, chunk_z), height_map);
        
        Ok(())
    }

    /// Manual flush of pending chunks
    pub async fn flush_chunks(&self) -> ZarrResult<()> {
        let mut pending = self.pending_chunks.lock().await;
        if pending.is_empty() {
            return Ok(());
        }
        
        let chunks_to_flush = std::mem::take(&mut *pending);
        drop(pending);
        
        self._flush_chunks(chunks_to_flush).await
    }

    /// Internal flush implementation - groups chunks by zarr region for real batching
    async fn _flush_chunks(&self, chunks: HashMap<(i32, i32), Vec<i64>>) -> ZarrResult<()> {
        // Group MC chunks by which zarr region they belong to
        let mut zarr_region_groups: HashMap<(i32, i32), Vec<((i32, i32), Vec<i64>)>> = HashMap::new();
        
        for ((chunk_x, chunk_z), height_map) in chunks {
            // Apply coordinate offset to handle negative coordinates
            let offset_x = chunk_x as i64 + self.coordinate_offset;
            let offset_z = chunk_z as i64 + self.coordinate_offset;
            
            // Validate coordinates are positive after offset
            if offset_x < 0 || offset_z < 0 {
                return Err(format!(
                    "Coordinate out of bounds: chunk ({}, {}) with offset becomes ({}, {})",
                    chunk_x, chunk_z, offset_x, offset_z
                ).into());
            }
            
            // Calculate which zarr region this MC chunk belongs to
            let region_x = (offset_x / self.zarr_region_size as i64) as i32;
            let region_z = (offset_z / self.zarr_region_size as i64) as i32;
            
            zarr_region_groups
                .entry((region_x, region_z))
                .or_insert_with(Vec::new)
                .push(((chunk_x, chunk_z), height_map));
        }
        
        // Now write entire zarr regions at once
        for ((region_x, region_z), chunks) in zarr_region_groups {
            let zarr_region_blocks = self.zarr_region_size * CHUNK_DIM as usize;
            let region_x_start = region_x as u64 * zarr_region_blocks as u64;
            let region_z_start = region_z as u64 * zarr_region_blocks as u64;
            
            // Create full zarr region data (filled with zeros initially)
            let mut zarr_data = vec![0i64; zarr_region_blocks * zarr_region_blocks];
            
            // Fill in the MC chunks we have
            for ((chunk_x, chunk_z), height_map) in chunks {
                // Apply offset and calculate local position within zarr region
                let offset_x = chunk_x as i64 + self.coordinate_offset;
                let offset_z = chunk_z as i64 + self.coordinate_offset;
                let local_x = (offset_x % self.zarr_region_size as i64) as usize;
                let local_z = (offset_z % self.zarr_region_size as i64) as usize;
                
                for x in 0..CHUNK_DIM as usize {
                    for z in 0..CHUNK_DIM as usize {
                        let chunk_idx = x * CHUNK_DIM as usize + z;
                        let region_x_pos = local_x * CHUNK_DIM as usize + x;
                        let region_z_pos = local_z * CHUNK_DIM as usize + z;
                        let zarr_idx = region_x_pos * zarr_region_blocks + region_z_pos;
                        
                        if zarr_idx < zarr_data.len() {
                            zarr_data[zarr_idx] = height_map[chunk_idx];
                        }
                    }
                }
            }
            
            // Write the entire zarr region in one operation
            let subset = ArraySubset::new_with_ranges(&[
                region_x_start..(region_x_start + zarr_region_blocks as u64),
                region_z_start..(region_z_start + zarr_region_blocks as u64),
            ]);
            
            let bytes: Vec<u8> = zarr_data.iter()
                .flat_map(|&val| val.to_le_bytes())
                .collect();
            
            // Validate data size before storing
            let expected_size = (zarr_region_blocks * zarr_region_blocks * 8) as usize; // 8 bytes per i64
            if bytes.len() != expected_size {
                return Err(format!(
                    "Data size mismatch: got {} bytes, expected {} bytes for zarr region ({}, {})",
                    bytes.len(), expected_size, region_x, region_z
                ).into());
            }
            
            self.array.store_array_subset(&subset, bytes)
                .map_err(|e| format!(
                    "Failed to store zarr region ({}, {}) at range {}..{}, {}..{}: {}",
                    region_x, region_z, region_x_start, region_x_start + zarr_region_blocks as u64,
                    region_z_start, region_z_start + zarr_region_blocks as u64, e
                ))?;
        }
        
        Ok(())
    }
}